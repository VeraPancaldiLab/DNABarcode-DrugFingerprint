---
title: "Inferring the MOA of new drugs through the analysis of heterogeneous response to treatment of different subpopulations of cancer cells"
author: "Bastien CHASSAGNOL and Vera PANCALDI and Luca GRUMULATO"
date: last-modified	
number-sections: true
toc: true
toc-depth: 3
lang: en-GB
# subject: "Cell-cell benchmark"
# keywords: ["cell-cell communication", "benchmark", "spatial transcriptomics", "single-cell"]
# bibliographic options
bibliography: DRB_fingerprints.bib
link-citations: true
highlight-style: github
filters:
  - highlight-text
# code options
execute:
  message: false
  warning: false
  error: false
# Table options
tbl-cap-location: bottom
format: 
  html:
    embed-resources: true
    theme:
      light: cosmo
      dark: cosmo
    sidebar: true
    lightbox: true # Allows to zoom out on figures.
    toc-expand: 2 # by default, level 2 is visible before scrolling
    comments: 
      hypothesis: true
    # code options
    code-fold: show
    code-link: true
    page-layout: full
    collapse: true
  # pdf:
  #   colourlinks: true
  #   cite-method: biblatex
  # docx:
  #   toc-title: Contents
editor: source
---

# Introduction: Cell lines DNA Barcoding

- DNA barcoding is also used for inferring the species abundancies in environement samples, just replace the notion of species by cell lines. Similar statistical issues, with *zero-inflated* distributions.

- Steps: 
  1. Transfection by virus.
  2. Clonal amplification, unique tagging per cell using MOI.
  3. Clone sizes are assumed to be proportional to the barcode abundances due to this 1-1 mapping of a barcode and a single cell.

- Pros DNA barcoding:
  - Better capture of cell population sizes
  - Better tracking of tagged clones.

- Cons DNA barcoding:
  - Lack of systematic reviews and benchmarks.
  - Mostly rely on bulk RNASeq analytical tools, not accounting for *drop-outs*. In particular, the assumptions that variance across tags is homogeneous, and abundancies follow a negative binomial distribution are quite controversial.
  

# Reproduce the analyses: setup configuration

I list below the R packages required to reproduce the analyses.

```{r}
#| label: setup

# data wrangling
library(dplyr)
library(tidyr)
library(stringr)
library(readr)
library(readxl)
library(purrr)

# reporting
library(flextable)
library(testthat)

# plotting
library(ggplot2)
library(ComplexHeatmap)
library(cowplot)
library(grid)

# auxiliary functions
source("R/utils.R")
today_date <- "2025-04-28"
# today_date <- format(Sys.Date(), "%Y-%m-%d")
```

# Data management

-   [Original Google Drive repository](https://drive.google.com/drive/folders/1pMSX4M4kHcDGxcsCzsMYHHM6VRvBHHUO).

    
```{r}
#| label: format-barcode-csv
#| echo: false
#| eval: false
barcode_files <- list.files("./data/barcode-counts/", 
                               pattern = "exp.*\\.xlsx", 
                               full.names = TRUE)

# change format + barcode name
barcode_matrices <- lapply(barcode_files, function(old_filename) {
  expression_matrix <- readxl::read_excel(old_filename) |> 
    rename(barcode_id = 1)
  
  new_filename <- old_filename |> basename() |> tools::file_path_sans_ext()
  
  readr::write_csv(expression_matrix, 
                   file = file.path("data", "barcode-counts", paste0(new_filename, ".csv")))
  
})
```

    
## Experiment selection and formatting {#sec-select-exp}

We early discarded the following expression files:

- ['Experiment Date': `130921` and 'Run Date': `101121` `exp130921_in vivo.csv`]{fg="red"}

```{r}
#| label: select-expression-130921
#| eval: false
#| echo: true

exp130921_expression <- readr::read_csv("data/barcode-counts/exp130921.csv") 
exp130921_1_expression <- readr::read_csv("data/barcode-counts/exp130921_1.csv") 
selected_dataset <- compare_dataframes(exp130921_expression, exp130921_1_expression)
```

[Conclusion: Keep `exp130921.csv`, and discard expression file `exp130921_1.csv`. The latter indeed exhibited fewer barcode IDs, while on the shared overlap, both datasets are identical.]{fg="red"}


```{r}
#| label: rename-erroneous-date-271221
#| eval: false
#| echo: false
#| lst-label: lst-rename-bad-date
#| lst-cap: Update labelling of latest experience.

exp271221 <- readr::read_csv("data/barcode-counts/exp271221.csv") 
exp271221_whole <- readr::read_csv2("old-data-backup/data-raw/barcode-counts/exp211221cor.csv") |> 
  rename(barcode_id = 1)
selected_dataset <- compare_dataframes(exp271221, exp271221_whole)

file.rename("data/barcode-counts/exp211221.csv", "data/barcode-counts/exp271221.csv") 
```

[In @lst-rename-bad-date, we rename expression file `exp211221.csv` into `exp271221.csv`, relying on date annotations.]{fg="red"}

```{r}
#| label: discriminate-200921-dose-response-standard
#| eval: true
#| message: true

exp200921_standard <- readr::read_csv("data/barcode-counts/exp200921.csv") 
exp200921_dose_response <- readr::read_csv("data/barcode-counts/exp200921_dose response osim.csv")


exp200921_global <- readr::read_tsv("data/barcode-counts/exp200921_whole exp.tabular") |> dplyr::rename(barcode_id="key")

# able to map all samples from global to either dose response or standard
setdiff(colnames(exp200921_global), 
        union(colnames(exp200921_dose_response), colnames(exp200921_standard)))

# all dose response variables can be found in whole experiment
setdiff(colnames(exp200921_dose_response), colnames(exp200921_global))
readr::write_csv(exp200921_global |> 
                   dplyr::select(dplyr::all_of(colnames(exp200921_dose_response))), 
                 "data/barcode-counts/exp200921_dose response osim.csv")

# 5 compounds (so 5*4=20) replicates are not avalaible in whole experiment
# waiting for confirmation before removing them
intersect(colnames(exp200921_standard), colnames(exp200921_global))
print("\n\n")
setdiff(colnames(exp200921_standard), colnames(exp200921_global))

cat(intersect(colnames(exp200921_standard), colnames(exp200921_global)), 
    file = "./data/logs/replicates_preserved_200921.txt", sep = "\n")
cat(setdiff(colnames(exp200921_standard), colnames(exp200921_global)), 
    file = "./data/logs/replicates_discarded_whole_200921.txt", sep = "\n")

# selected_dataset <- compare_dataframes(exp200921_standard, exp200921_dose_response)
```

[While the *dose-response* experience `exp200921_dose response osim` exhibits distinct biological objective, it turned out that the 4 control biological replicates are shared with classical drug response comparison `exp200921.csv`. However, **the number of sequenced barcode IDs differ in both experiences!!** In addition, note that `exp200921.csv` has been tagged as discarded, except for the Controls!!]{fg="red"}
  
Code snippets in @lst-281022 and @lst-281022-time-course aim at replacing unprecise labelling of barcode IDs (no concentration, no time duration, ...) in the 2022 batch experiences.

```{r}
#| label: select-expression-281022
#| eval: false
#| echo: true
#| lst-label: lst-281022
#| lst-cap: Update labelling of latest experience.

# extract old colnames
exp281022_expression <- readr::read_csv("data/barcode-counts/exp281022.csv") 
# cat(colnames(exp281022_expression), file = "281022_old_labels.txt", sep = "\n")

# extract mapping old_names with new_names
mapping_old_new_281022 <- readxl::read_excel("data/Table of compounds_whole_2025-04-28.xlsx", 
                                             sheet = "Sample Mapping 281022")

labels_old_new_281022 <- setNames(object = mapping_old_new_281022$OLD_replicate_label, 
                                  nm = mapping_old_new_281022$NEW_replicate_label)

# apply change labelling
exp281022_expression <- exp281022_expression |> 
  dplyr::rename(dplyr::all_of(labels_old_new_281022))

readr::write_csv(exp281022_expression, "data/barcode-counts/exp281022.csv")
```


```{r}
#| label: select-expression-281022-time-course
#| eval: false
#| echo: true
#| lst-label: lst-281022-time-course
#| lst-cap: Update time course experiences.

exp281022_expression_timecourse <- readr::read_csv("data/barcode-counts/exp281022_time course.csv") 
# cat(colnames(exp281022_expression_timecourse), file = "281022_timecourse_old_labels.txt", sep = "\n")

# update colnames ----

mapping_old_new_time_course <- read_excel("data/Table of compounds_whole_2025-04-28.xlsx", 
                                     sheet = "Sample Mapping Time Course")

labels_old_new_time_course <- setNames(object = mapping_old_new_time_course$OLD_replicate_label, 
                                       nm = mapping_old_new_time_course$NEW_replicate_label)

# apply change labelling
exp281022_expression_timecourse <- exp281022_expression_timecourse |> 
  dplyr::rename(dplyr::all_of(labels_old_new_time_course))

readr::write_csv(exp281022_expression_timecourse,
                 "data/barcode-counts/exp281022_time course.csv")

# To evaluate whether controls are different between experiment 281022 and 281022 Time Course
controls_exp281022 <- exp281022_expression |> 
  select(key_barcode, dplyr::matches("^ctl")) |> 
  dplyr::inner_join(exp281022_expression_timecourse |> 
                      select(key_barcode, dplyr::matches("^Ctrl")),
                    by="key_barcode")
cor(controls_exp281022$ctl2, controls_exp281022$Ctrl2) # they strongly correlate with each other, but not a perfect match -> controls are indeed distinct

```

```{r}
#| label: select-expression-300821
#| eval: false
#| echo: false


exp300821 <- readr::read_csv("data/barcode-counts/exp300821.csv") 

# fake additional colnames, containing only missing colnames
exp300821_whole <- readr::read_csv2("old-data-backup/data-raw/barcode-counts/exp300821.csv") |> 
  rename(barcode_id = 1)
selected_dataset <- compare_dataframes(exp300821, exp300821_whole)

setdiff(exp300821$barcode_id, exp300821_whole$barcode_id)
cor(exp300821$Osimer4_100n_exp300821_run290921_08, exp300821_whole$Osimer4_100n_exp300821_run290921_08)

readr::write_csv(exp300821, "data/barcode-counts/exp300821.csv")
```


<!-- ### Additional comments -->

<!-- - `exp130921_in_vivo`: cells injected in living mice (control versus osermintinimb). -->
<!-- - `exp281022`: control, protac, cetuximab and gefitinib ($n=4$ replicates each time) were sampled in T25 flasks. P42 and P43 correspond to cells grown without treatment for 42 or 43 passages, and were used for evaluating *barcode drift* technical artefact. -->
<!-- - `exp281022_time course`: time course experiment. -->

### TODO and TOKEEPINMIND: barcode counts

Things to consider:

- [Format used is **French csv**, instead of international CSV. CSV means for 'comma-separated value', maybe consider switching [Excel settings](https://support.microsoft.com/en-us/office/import-or-export-text-txt-or-csv-files-5250ac4c-663c-47ce-937b-339e391393ba).]{fg="red"}
- [I rename the first colname of each expression file as `barcode_id`, storing unequivocal tag.]{fg="red"}
- [Some barcode counts exhibit final line `index`, for which reason?]{fg="red"}
<!-- - Some barcode counts matrices, such as `exp220322.csv` and `exp070222.csv` showcase a final line paired with `index` value, or even a full empty line, such as in `exp200921.csv`. Not really an issue after merging all barcode files. -->

### Overview of barcode samples


```{r}
#| label: tbl-overview-barcode-samples
#| tbl-cap: "Overview of the Barcoding Batches."
#| cache: false
#| tbl-subcap:
#|   - "Kept Batches"
#|   - "Removed batches"
#| layout-ncol: 2

# 1) summarise kept batches ----
barcode_files <- list.files("./data/barcode-counts/",
                            pattern = "exp.*\\.csv",
                            full.names = TRUE)
barcode_summaries <- purrr::map(barcode_files, function(filename) {
  barcode_counts <- readr::read_csv(filename, show_col_types = FALSE)
  experience_name <- filename |> basename() |> tools::file_path_sans_ext()
  experience_summary <- tibble::tibble(`Experience Name`= experience_name,
                                       Features= paste0("Barcode matrix contains: ",
                                                        nrow(barcode_counts),
                                                        " unique barcode IDs, and ",
                                                        ncol(barcode_counts)-1, " replicates."))
  return(experience_summary)
}) |>
  purrr::list_rbind()
flextable::flextable(barcode_summaries) |> 
  bold(part="header")

openxlsx::write.xlsx(barcode_summaries, 
                     file = "./data/kept_experiences_overview.xlsx")

# 2) summarise discarded batches ----

discarded_files <- list.files("./data/barcode-counts/temp discarded/",
                            pattern = "exp.*\\.csv",
                            full.names = TRUE)

# change format + barcode name
barcode_discarded_summaries <- purrr::map(discarded_files, function(filename) {
  barcode_counts <- readr::read_csv(filename, show_col_types = FALSE)
  experience_name <- filename |> basename() |> tools::file_path_sans_ext()
  experience_summary <- tibble::tibble(`Experience Name`= experience_name,
                                       Features= paste0("Barcode matrix contains: ",
                                                        nrow(barcode_counts),
                                                        " unique barcode IDs, and ",
                                                        ncol(barcode_counts)-1, " replicates."))
  return(experience_summary)
}) |>
  purrr::list_rbind()

openxlsx::write.xlsx(barcode_discarded_summaries, 
                     file = "./data/discarded_experiences_overview.xlsx")

flextable::flextable(barcode_discarded_summaries) |> 
  bold(part="header")
```
In @tbl-overview-barcode-samples, we list the final selection of `{r} ncol(barcode_summaries)` barcode experiences considered so far in the analysis, while `{r} ncol(discarded_files)` were not considered for inclusion at all.


## Phenotypes {#sec-metadata-phenotypes}

- [General question: what's the difference between `Run Date` and `Experiment Date`?]{fg="red"}

Most of the R instructions reported in this section aim at rendering the `Table of compounds` file compliant with the `tidy` format, see @tip-tidyformat. These data-wrangling operations are split in 4 steps:

1. In @lst-process-pheno1, we remove columns that are not useful for pairing counts and phenotype information, nor for the statistical design, correct for wrongly reported concentrations, and deal with merged Excel cells that generate spurious missing values.

2. In @lst-process-pheno2, we retrieve from each individual barcode counts profile, the individual replicates ID identifying unequivocally each replicate. We set apart replicates that could be mapped back to original `Table of compounds` file.

3. In @lst-process-pheno3, we merge together in a comprehensive phenotype table the short `Samples_ID` (short term referring to drugs listed in `Samples` column), the full `Samples` (complete name of drugs) and map each of them to its complete list of replicates (from 2 to 8). We save the output in `data-derived`.

4. Finally, in @lst-check-pheno-data-1 and @lst-check-pheno-data-2, we perform several posterior *data integrity checks* to verify if i) we were able to map each individual replicate ID to its original ID in `Table of compounds`, and ii) if the number of replicates reported in [`Table of compounds`](data/Table of compounds_whole_2025-04-28.xlsx) was consistent with the number of barcode profiles.

```{r}
#| label: process-pheno1
#| lst-label: lst-process-pheno1
#| lst-cap: Coerce original Excel file reporting experimental design to tidy format.

pheno_colnames <- c(Level1 = "Pathways", Level2 = "...2", 
                    Samples = "Samples", Replicates = "Replicates",
                    Concentrations = "Concentrations", Date = "Experiment date",
                    `Run date` = "Run date", Duration="Duration", 
                    Kept="Kept", Comments = "Comments")

# prune irrelevant colnames ----
# data/Table of compounds_stringent_2025-04-18.xlsx
pheno_data <- readxl::read_xlsx("data/Table of compounds_whole_2025-04-28.xlsx",
                                sheet = "Experimental Design",
                                col_types = c(rep("text", 3), "numeric", 
                                              rep("text", 4), "logical", "text")) |>
  rename(all_of(pheno_colnames)) |>
  # tidyr::drop_na(Replicates, Date) |> # 3 cells seem erroenously merged
  # missing_rows <- which(is.na(pheno_data$Replicates) & is.na(pheno_data$Date))
  tidyr::fill(Level1, Level2, .direction = "down")

pheno_data_discarded <- pheno_data |> 
  dplyr::filter(!Kept)
openxlsx::write.xlsx(pheno_data_discarded, 
                     file = paste0("data/logs/Removed_compounds_", today_date,".xlsx"), 
                     asTable = TRUE)

# Add batches and correct for fused Excel cells ----

pheno_data <- pheno_data |> 
  # extract last 6 numbers, as the only ones used for identification of samples downstream
  dplyr::mutate(Samples = if_else(grepl("Control", Level2, ignore.case = TRUE),
                                  Level2, Samples),
                `Run date` = stringr::str_sub(`Run date`, -6, -1),
                Batch_ID= paste0("exp", Date)) |> 
  # Date = if_else(Samples %in% "SBI-0206965", "181021", Date),
  # `Run date` = if_else(Samples %in% "SBI-0206965", "251121", `Run date`)) |>
  tidyr::fill(Samples, .direction = "down") |>
  # remove unwanted replicates
  dplyr::filter(Kept) |> 
  dplyr::mutate(Batch_ID = dplyr::if_else(Comments %in% c("Dose Response"), 
                                          "exp200921_dose response osim", Batch_ID)) |> 
  dplyr::mutate(Batch_ID = dplyr::if_else(Comments %in% c("Time Course"), 
                                          "exp281022_time course", Batch_ID)) |> 
  dplyr::select(- Comments, -Kept) |> 
  dplyr::distinct(.keep_all = TRUE) # remove duplicates

# format concentrations, creating both 'Concentrations_ID' and 'Concentrations' in Moles ----
pheno_data <- pheno_data |> 
  dplyr::mutate(Concentrations=if_else(Samples=="Control", "000 uM", Concentrations)) |> 
  dplyr::mutate(Concentrations=if_else(Samples=="Osimertinb+sorafenib", "015 uM", Concentrations)) |>
  tidyr::separate_wider_delim(Concentrations, delim = " ", names = c("Concentrations Value", "Unit"), cols_remove = FALSE) |> 
  dplyr::rename(OLD_Concentrations=Concentrations) |> 
  dplyr::mutate(Unit = case_when(
    Unit %in% c("uM", "µM", "µg/ml") ~ "u",
    Unit == "nM" ~ "n", 
    Unit %in% c("mg/kg", "mM") ~ "m", 
  )) |> 
  dplyr::mutate(Concentrations=parse_number(`Concentrations Value`, trim_ws = TRUE, 
                                            locale = locale(decimal_mark = ",", grouping_mark = "."))) |> 
   dplyr::mutate(Concentrations = case_when(
    Unit == "u" ~ Concentrations *10^-6,
    Unit == "n" ~ Concentrations *10^-9, 
    Unit == "m" ~ Concentrations *10^-3)) |> 
  dplyr::mutate(`Concentrations Value`=parse_number(`Concentrations Value`, trim_ws = TRUE, 
                                            locale = locale(decimal_mark = ",", grouping_mark = ".")) |> 
                  format_concentrations()) |> 
  tidyr::unite(col ="Concentrations_ID", `Concentrations Value`, Unit, sep = "")

# format Durations, creating both 'Duration_ID' and 'Duration' as an integer value in Days ----
pheno_data <- pheno_data |> 
  # extract last 6 numbers, as the only ones used for identification of samples downstream
  dplyr::mutate(Duration_ID = Duration,
                Duration = parse_number(Duration, trim_ws = TRUE, 
                                        locale = locale(decimal_mark = ".")), 
                Duration = dplyr::if_else(grepl("m$",  Duration_ID),
                                          Duration*30, Duration)) 
  
```

::: {.callout-important title="Metadata inconsistencies and Conversion to UIC Units" collapse="true"}

<!-- - [For drug `SBI-0206965`, both `Date` and `Run date` were wrongly reported (either not stored in the proper barcode counts profile, or mistyped in `Table of Compounds`).]{fg="red"} -->

- [In `Run date`, when there was a range instead of a fixed Date, I kept only the end of the interval, as used for labelling colnames in barcode counts.]{fg="red"}

- [Homogenise units for the `Concentrations` colname: convert everything to moles.]{fg="red"}

- [Homogenise units for `Date` and `Run date`, using international ISO 8601 format: `YYYY-MM-DD`]{fg="red"}

:::

In @lst-process-pheno2 and @lst-process-pheno3, we identify which raw barcode counts matrices avalaible in folder `barcode-counts` are not reported in the global experimental dataset, and reciprocally. Finally, we only **keep experiences that are both reported in `Table of compounds` and present in `barcode-counts` folder.**.

```{r}
#| label: process-pheno2
#| lst-label: lst-process-pheno2
#| lst-cap: Identify shared experience IDs between experimental design table (`Table of compounds`), and barcode-count counts profiles.

# keep only relevant files 
barcodes_filenames <- list.files("./data/barcode-counts/",
                                pattern = "^exp.*\\.csv$")

barcodes_IDs <- readxl::read_xlsx("data/Table of compounds_whole_2025-04-28.xlsx",
                                       sheet="Batch Mapping") |> 
  select(Filename, Batch_ID) |> 
  filter(!Batch_ID %in% c("exp130921_in vivo")) |> 
  mutate(Filename =paste0(Filename, ".csv"), 
         Date = stringr::str_extract(Batch_ID, "(?<=^exp)[[:digit:]]{6}"))

```


<!-- ::: {.callout-important title="Missing information on Table of Compounds and missing Experiences"} -->

<!-- We note the following inconsistencies: -->

<!-- 1. Using ``{r} setdiff(barcodes_filenames, barcodes_IDs$Filename)``, we note that the following files present in `data-raw` will be excluded: `{r} setdiff(barcodes_filenames, barcodes_IDs$Filename)`. For details, report to @sec-select-exp. -->

<!-- 2. Using ``{r} setdiff(barcodes_IDs$Filename, barcodes_filenames)``, we certified that all remaining filenames in the folder were properly mapped. -->

<!-- 3. Using ``{r} setdiff(pheno_data$Batch_ID, barcodes_IDs$Batch_ID)``, all experiments `{r} paste(setdiff(pheno_data$Batch_ID, barcodes_IDs$Batch_ID), collapse = ", ")` reported in original `Table of Compounds` could have been mapped back to an existing experiment, once `Control - time zero` have been discarded.  -->

<!-- ::: -->

To generate the final phenotype dataset, we only keep experiences that were reported both in `Table of Compounds` and folder `barcode-counts` in @lst-process-pheno3. [**Please ensure that `Drugs Mapping` sheet in [`Table of compounds`](data/Table of compounds_whole_2025-04-28.xlsx) is correct.**]{fg="red"}

```{r}
#| label: process-pheno3
#| lst-label: lst-process-pheno3
#| lst-cap: Map each drug to its full list of replicates. Generate a `Batch_ID` to unequivocally ientify each run of experiences, and convert all run Dates to their international format.

# step 1) keep only shared Batch_ID experiences ----
barcodes_IDs <- barcodes_IDs |>
  dplyr::semi_join(pheno_data, by="Batch_ID")
# exp201021 is discarded, that's normal, we just want to keep the controls!!
pheno_data <- pheno_data |>
  dplyr::semi_join(barcodes_IDs, by="Batch_ID")

# step 2) extract individual replicate IDs directly from sample experiences ----
# Filename <- "exp281022.csv"; Batch_ID <- "exp281022"; Date <- "281022"
ID_mapping <- purrr::pmap(barcodes_IDs, function(Filename, Batch_ID, Date) {
  # build path
  counts_path <- file.path("data","barcode-counts", 
                               paste0(Filename))
  
  # extract replicate names (reading the header)
  replicates_ID <- strsplit(readLines(counts_path, n = 1), split = ",")[[1]] |>
    str_subset("\\S") |> # remove empty strings and 'key'
    str_subset("barcode_id", negate = TRUE)

  # ID: combination of letters, numbers and -, followed by '[1-4]_', starting the sample's name
  dataset_ID <- tibble::tibble(Batch_ID = Batch_ID, 
                               Date = stringr::str_extract(replicates_ID,
                                                           "(?<=_exp)[[:alnum:]]{6}"),
                               Replicates_ID = replicates_ID,
                               `Run date` = stringr::str_extract(replicates_ID,
                                                                 "(?<=_run)[[:alnum:]]{6}(?=_)"),
                               Concentrations_ID = stringr::str_extract(replicates_ID, 
                                                                        "(?<=[1-8]_)[[:digit:]]{3}[gmnux]{1}(?=_exp)")) |> 
    # account for second notation syntax for concentrations
    dplyr::mutate(Concentrations_ID = if_else(is.na(Concentrations_ID), 
                                              stringr::str_extract(replicates_ID,
                                                                   "(?<=[1-8]_)[0-9]{1}\\.[0-9]{2}[gmnux]{1}(?=_exp)"),
                                              Concentrations_ID))
  
  # Deal with two time notations
  if (grepl("Time",Batch_ID, ignore.case = TRUE)) {
    dataset_ID <- dataset_ID |> 
      dplyr::mutate (Samples_ID = stringr::str_extract(replicates_ID,
                                                       "(?<=_)[[[:alnum:]]\\-]+(?=[1-8]_)"), 
                     Duration_ID=stringr::str_extract(replicates_ID,
                                                       "^[[[:alnum:]]\\.]{1,3}[dm](?=_)"))
  }
  else {
      dataset_ID <- dataset_ID |> 
      dplyr::mutate (Samples_ID = stringr::str_extract(replicates_ID,
                                                       "^[[[:alnum:]]\\-]+(?=[1-8]{1}_)"), 
                     Duration_ID="9d")
  }
                                 
  return(dataset_ID)
}) |>
  purrr::list_rbind()

# deal with specific p42 and p43
ID_mapping <- ID_mapping |> 
  dplyr::mutate(Samples_ID = if_else(grepl("^p42", Replicates_ID), "p42", Samples_ID), 
                Samples_ID = if_else(grepl("^p43", Replicates_ID), "p43", Samples_ID))

# remove irrelavant samples
ID_mapping <- ID_mapping |> 
  dplyr::filter(!grepl("^p42|^p43|^CTRL", Replicates_ID))


# detect miscatech samples
# ID_mapping_missing <- ID_mapping |>
#   filter(if_any(everything(), is.na))

# step 3) map short drug IDs with full drug names ----
mapping_compounds <- readxl::read_xlsx("data/Table of compounds_whole_2025-04-28.xlsx",
                                sheet = "Drugs Mapping")
ID_mapping <- ID_mapping |> 
  inner_join(mapping_compounds, by = "Samples_ID")
# test <- ID_mapping |> anti_join(mapping_compounds, by = "Samples_ID")
# test2 <- mapping_compounds |> anti_join(ID_mapping, by = "Samples_ID")
```

<!-- - `Alisertib` experiment from `Date: 220322` is assigned with a `050u` concentration in *counts*, against `050n` in *phenotype data*. -->
<!-- - `Sorafenib` experiment from `Date: 151121` is assigned with a `005u` concentration in *counts*, against `004u` in *phenotype data*. On the other hand, `Sorafenib` experiment from `Date: 300821` with a `005u` concentration in *phenotype data* has been removed in *counts*. -->
<!-- - `X13271` with concentration `005u`, both run on `Run Date: 180821`, have been discarded in *counts*. -->

Finally, in @lst-check-pheno-data-1, we map all replicates (one column in a given barcode count matrix) to its comprehensive metadata descripion (stored in `Tables of compounds`), using as **primary and foreign keys** the following 6 variables: `Batch_ID`,  `Samples`, `Duration_ID`, `Run date`, `Date`, and `Concentrations_ID`.

```{r}
#| label: tbl-check-pheno-data
#| lst-label: lst-check-pheno-data-1 
#| lst-cap: "Extract discarded replicates"
#| tbl-cap: Identify set of experiences reported in `Table of compounds` that could not be mapped against their corresponding Barcode counts replicates. We check this information with an `anti_join` between original *phenotype* and *counts*, returning samples from `Table of Compounds` that could not have been mapped back.

pheno_data_unmapped <- dplyr::anti_join(pheno_data,
                                     ID_mapping,
                                     by = c("Batch_ID",  "Samples", "Duration_ID",
                                            "Run date", "Date", "Concentrations_ID"))

pheno_data_unmapped |> 
  arrange(Samples, Date, Concentrations_ID) |> 
  flextable() |> 
  bold(part = "header")

replicates_discarded <- dplyr::anti_join(ID_mapping, pheno_data,
                                     by = c("Batch_ID",  "Samples", "Duration_ID",
                                            "Run date", "Date", "Concentrations_ID"))
readr::write_csv(replicates_discarded, 
                 file = paste0("data/logs/discarded_replicates_", today_date,".csv"))
```

In @tbl-check-pheno-data-2, we check discrepancies between the number of barcode replicates reported in `Table of compounds` with the number of replicates avalaible in barcode counts matrices. [And it turned out that `compound`: XAV-939, `Date`: 220322, is reported with 4 replicates, while only 2 could be found in [`exp220322` barcode count matrix](./data/barcode-counts/exp220322.csv)]{fg="red"}

```{r}
#| label: tbl-check-pheno-data-2
#| lst-label: lst-check-pheno-data-2
#| lst-cap: Secound round of data wrangling quality controls on *counts* versus *phenotype data*, focusing on divergent number of replicates.
#| tbl-cap: Secound round of data wrangling quality controls on *counts* versus *phenotype data*, focusing on divergent number of replicates.

replicate_inconsistencies <- pheno_data |> 
  dplyr::inner_join(ID_mapping,
                    by = c("Batch_ID", "Date", 
                           "Samples", "Run date",
                           "Concentrations_ID", "Duration_ID")) |>
  group_by(Batch_ID, Samples, Date, Replicates, Concentrations_ID,Duration_ID) |>
  summarise(n=n()) |>
  filter(n!=Replicates)

flextable(replicate_inconsistencies) |>
  autofit() |>
  bold(part = "header")

```

### Phenotypes metadata: Final Table generation

By applying all the formatting analyses reported in @sec-metadata-phenotypes, we generate in @tbl-phenotype-metadata-saving a global metadata phenotypic table, adhering to tidy principles [@tip-tidyformat]. [Note that I renamed some colnames, better fitting usual naming conventions:        `Level1` by `Pathway`, `Level2` by `MoA` (for mechanism of action ), and `Samples` by `Compound`.]

::: {#tip-tidyformat .callout-tip title="Tidy Data Format (Key Principles)" collapse="true"}

1. **Each variable has its own column**.
2. **Each observation has its own row** – Every row corresponds to one observation.
3. **Each value has its own cell** – Each cell contains a single, unique value.

This format streamlines data wrangling, and generally speaking, data analysis and visualisation. In other words, prefer simpler `CSV` formats for the experimental design, and avoid *cell fusion* in Excel documents. Finally, for the formatted tabular representation in documentations, I use `flextable`.

:::

```{r}
#| label: tbl-phenotype-metadata-saving
#| lst-label: lst-save-pheno-metadata
#| lst-cap: Save the final metadata annotations for barcodes, using Today's date for historical versioning.

# step 4) Use international date formats ----
pheno_data_formatted <- pheno_data |> 
  dplyr::inner_join(ID_mapping,
                    by = c("Batch_ID", "Date", 
                           "Samples", "Run date",
                           "Concentrations_ID", "Duration_ID")) |> 
  dplyr::select(Batch_ID,
                # samples names
                Pathway=Level1, MoA=Level2,
                Compound=Samples, Samples_ID,
                Replicates, Replicates_ID,
                # replicate features
                Concentrations, Concentrations_ID, 
                Date, `Run date`, Duration, Duration_ID) |> 
  # convert to ISO 1860 Date format
  mutate(Date = lubridate::dmy(Date) |> format(),
         `Run date`= lubridate::dmy(`Run date`))

# step 5) save the tidy phenotype metatada table ----
readr::write_csv(pheno_data_formatted,
                 file = paste0("data/pheno_data_metadata_", today_date,".csv"))

flextable::flextable(head(pheno_data_formatted)) |> 
  autofit() |> 
  bold(part="header")
```


**Conclusion**: on a total of `{r} nrow(ID_mapping)`, `{r} nrow(pheno_data_formatted)` replicates are kept, and `{r} nrow(replicates_discarded)` replicates were filtered out, based on Luca's guidelines.

# Methods

## Preprocessing barcode counts{#sec-batch-processing}

**Code colour**: [Luca's protocol]{fg="blue"}, and [Bastien's comments and perspectives]{fg="red"}

### **Background Noise Removal** and `SummarizedExperiment` stantardised storage

-   [Eliminate barcodes for which the combined counts of the 4 controls per barcode are below a given threshold, here `5`.]{fg="blue"} [Temporaly exclude file `./data/barcode-counts/exp200921_dose response osim.csv`, as the filtering does not return any barcode.]{fg="red"}

```{r}
#| label: background-filtering
#| lst-label: lst-background-filtering
#| lst-cap: Remove background noise.

barcode_files <- list.files("./data/barcode-counts/",
                            pattern = "exp.*\\.csv",
                            full.names = TRUE)
thresh_background <- 4
pheno_data <- readr::read_csv("./data/pheno_data_metadata_2025-04-17.csv")
# filename <- barcode_files[6]

# explicit and data-sound but unefficient code
barcode_counts_aggregated <- purrr::map(barcode_files, function(filename) {
  barcode_counts <- readr::read_csv(filename)
  experience_name <- filename |> basename() |> tools::file_path_sans_ext()
  replicates_names <- setdiff(colnames(barcode_counts), "barcode_id")
  
  ## Remove lowly expressed barcodes ----
  control_index <- pheno_data |> 
    dplyr::filter(Batch_ID==experience_name & 
                    Compound=="Control" &
                    Replicates_ID %in% replicates_names) |> 
    dplyr::pull(Replicates_ID)
  signicant_barcodes_index <- which(rowSums(barcode_counts[, control_index]) > thresh_background)
  filtered_barcode_counts <- barcode_counts[signicant_barcodes_index,]
  return(filtered_barcode_counts)
}) |> 
  purrr::reduce(~ inner_join(.x, .y, by = "barcode_id"))
```


-   [Density plots to evaluate the relevance of this threshold, `HTSFilter`, comparison with existing filtering approaches, ... But removing noise is indeed a great point!!, especially with the original high number of barcodes. However, why only performing this operation on control cases?]{fg="red"}

### **Normalisation**

-   [Normalize barcodes so that the total number of counts per sample is 100 000]{fg="blue"}

```{r}
#| label: cpm-normalisation
#| lst-label: lst-cpm-normalisation
#| lst-cap: Normalise by $100000$.
replicates_names <- setdiff(colnames(barcode_counts_aggregated),
                            "barcode_id") # alternatively, select numeric values

barcode_counts_aggregated <- barcode_counts_aggregated |> 
  mutate(across(
    .cols = where(is.numeric),                     
    .fns = ~ (.x / sum(.x)) * 1e5
  ))
```


-   [Close to two existing normalisation methods: **Counts Per Million (CPM)** which additionally scales raw counts by total library size and multiplies by $1,000,000$ and **Total Count Scaling (TCS)**: Scales raw counts by the total number of reads (or mapped reads) in each sample, then multiplies by a fixed number (e.g., 100,000). Would compare other normalisation approaches + ignore biological or technical biases + generate MA plots for verifying the mean-variance correction trend + not suitable for DEGs analyses. Apply concatenation phase of all samples before normalising. [Preprocessing and normalisation functions for transcriptomics: a general overview](https://www.linkedin.com/posts/%F0%9F%8E%AF-ming-tommy-tang-40650014_rna-seq-normalization-what-you-need-to-activity-7313919533780516864-6jFm)]{fg="red"}

### **Differential analyses** and Binarisation

1. [Calculate mean of the 4 controls, followed by **Differential Represented Barcode Analysis**: binarise each replicate, assining a 1 if Fold change is above 3 with respect to Mean value, and 0 otherwise.]{fg="blue"} [Batch effect correction for integrating across batches? As Vera emphasised it out, why keeping only positive values? Negative are also interesting, otherwise, we will bias towards drugs having a positive fitness. Why not pairing $p$-values and fold-change (considering indeed really small sample sizes)? All these operations can be run within one step using `lm`, and a model as such, $\text{Expr} \sim 0 + \text{Gene} + \text{Batch} + \text{Drug}$, with a `contr.treatment` design matrix [^1]]{fg="red"}.

```{r}
#| label: differential-barcode-analysis
#| lst-label: lst-differential-barcode-analysis
#| lst-cap: Compute the averaged basal value, .

barcode_counts_long <- barcode_counts_aggregated |> 
  tidyr::pivot_longer(-barcode_id, names_to = "Replicates_ID", values_to = "Barcode_Counts") |> dplyr::inner_join(pheno_data, by="Replicates_ID") |> 
  dplyr::select(-Pathway, -MoA, -Samples_ID, -Replicates, -Concentrations_ID, -Duration_ID)
threshold_FC <- 3

# Step 1: Compute control means per barcode_id and Batch ----
control_means <- barcode_counts_long |>
  filter(Compound == "Control") |>
  group_by(Batch_ID, barcode_id) |>
  summarise(Control_Mean = mean(Barcode_Counts), .groups = "drop")

# Step 2: Join control mean to full dataset ----
barcode_counts_discretised <- barcode_counts_long |>
  left_join(control_means, 
            by = c("Batch_ID","barcode_id"))

# Step 3: Compute Fold Change and discretise ----
barcode_counts_discretised <- barcode_counts_discretised |>
  filter(Compound != "Control") |>
  mutate(Diff = Barcode_Counts - Control_Mean,
    Barcode_Counts = if_else(Diff > threshold_FC, 1, 0)) |>
  # Final cleanup 
  select(-Diff, -Control_Mean)  |> 
  dplyr::mutate(Barcode_Counts=as.logical(Barcode_Counts))
```


[^1]: [Really wants to try the **Helmert contrast** to evaluate drug dose response, or time-course studies. Also For repeated measures across time, see [One Way repeated measure ANOVA in R](https://www.r-bloggers.com/2025/02/one-way-repeated-measure-anova-in-r/)]{fg="red"}

### **Derivation of drug fingerprints**:

[Aggregate by drug and experience.]{fg="blue"} [Efficient computation of binary values with a logical `AND` + perform **sensitivity analyses** to evaluate the impact of threshold variation.]{fg="red"}


```{r}
#| label: drug-fingerprint-estimation
#| lst-label: lst-drug-fingerprint-estimation
#| lst-cap: "Aggregate at the drug level, assigning a 1 if, and only if, barcode cell lines were found significant in a given drug."

barcode_counts_discretised_drug <- barcode_counts_discretised |> 
  group_by(Batch_ID, Concentrations, Duration, Date, `Run date`, barcode_id, Compound) |> 
  summarise(Barcode_Counts = all(Barcode_Counts == 1), .groups = "drop")


barcode_counts_discretised_replicates <- barcode_counts_discretised |> 
  select(-Concentrations, -Duration, -Date, -`Run date`, -Batch_ID, -Compound) |> 
  tidyr::pivot_wider(names_from = Replicates_ID, values_from = Barcode_Counts)
saveRDS(barcode_counts_discretised_replicates, 
        file="results/fingerprints_replicates_level.rds")

barcode_counts_discretised_drug <- barcode_counts_discretised_drug |> 
  dplyr::mutate(Compound=paste0("Drug: ", Compound, " with Batch: ", Batch_ID,
                                ", Conc: ", Concentrations, " and Days: ", Duration)) |> 
  select(-Concentrations, -Duration, -Date, -`Run date`, -Batch_ID) |> 
  tidyr::pivot_wider(names_from = Compound, values_from = Barcode_Counts)

saveRDS(barcode_counts_discretised_drug, 
        file="results/fingerprints_compounds_level.rds")
```


## Compute Drug Similarities

1. [Actual computation of the correlation matrix using `stats::cor`.]{fg="blue"} [Yet, we have to try other correlation methods on the continuous space, or consider other metrics if working on the discrete space. Unbalanced Wassertein distance, or relatives, for distinct input and output dimensions.]{fg="red"}

```{r}
#| label: convert-to-matrices
barcode_replicates_mat <- barcode_counts_discretised_replicates |> 
  tibble::column_to_rownames(var="barcode_id") |> 
  as.matrix()
cor_replicates <- cor(barcode_replicates_mat)


barcode_drugs_mat <- barcode_counts_discretised_drug |> 
  tibble::column_to_rownames(var="barcode_id") |> 
  as.matrix()
cor_drugs <- cor(barcode_drugs_mat)
```


2. Plot weighted undirected graphs with `igraph::graph_from_adjacency_matrix`
    
## Compute Barcode aka Cell Lines Similarities

[To select barcodes highly correlated within each other, must correlate, with $CC > 0.8$ with at least 4 others.]{fg="blue"}.

# Results

## Correlation Heatmaps

### Tutorials for generating Heatmaps

- [Blend of useful resources on generating Heatmaps](https://www.linkedin.com/posts/%F0%9F%8E%AF-ming-tommy-tang-40650014_making-a-heatmap-is-an-essential-skill-for-activity-7317543424511868928-F25t):

- [Mapping quantitative data to color](https://www.nature.com/articles/nmeth.2134), from @gehlenborg2012nm:
  
- **Real-Life Examples** with `ComplexHeatmap`: 

  - [Heatmap with transcriptomic expression](https://rpubs.com/crazyhottommy/heatmap_demystified)
  - [Heatmap with genomic expression](https://jokergoo.github.io/ComplexHeatmap-reference/book/more-examples.html)
  - [Genome-level Heatmap](https://jokergoo.github.io/ComplexHeatmap-reference/book/genome-level-heatmap.html)
  - [Plotting large heatmaps in R](https://gdevailly.netlify.app/post/plotting-big-matrices-in-r) and [*Rasterisation*, with thousands of genes to be considered](https://jokergoo.github.io/2020/06/30/rasterization-in-complexheatmap).
  
- **Real-life examples** with [`ggplot2`](https://github.com/theislab/atlas-feature-selection-benchmark/blob/b89fc0f66747062e6e1b4b35bd392b27ad035295/analysis/R/plotting.R#L8)

- **Real-Life examples** with [`funkyheatmap` CRAN package](https://funkyheatmap.github.io/)

### Heatmap drugs

- Heamaps at the drug and replicate level are reported in @fig-generate-heatmap-drug.

```{r}
#| label: fig-generate-heatmap-drug
#| fig-cap: "Heamaps at the drug and replicate level."
# define clustering colours
col_cor_scale <- circlize::colorRamp2(c(-1, 0, 1), c("blue", "white", "red"))

heatmap_drugs <- ComplexHeatmap::Heatmap(cor_drugs, col=col_cor_scale)
heatmap_replicates <- ComplexHeatmap::Heatmap(cor_replicates, col=col_cor_scale)

p1 <- grid.grabExpr(ComplexHeatmap::draw(heatmap_drugs))
p2 <- grid.grabExpr(ComplexHeatmap::draw(heatmap_replicates))
heatmaps_global <- plot_grid(p1, p2, nrow = 1, labels = c("A", "B"))
ggsave("figures/heatmap_fingerprints.pdf", 
       heatmaps_global, dpi = 600)

heatmaps_global
```

# Perspectives

## Pair Single Cell with (or without) Drug fingerprints

- [Single cell and drug response largest database](https://www.linkedin.com/posts/independent-data-lab_check-out-the-preprint-activity-7300471798029012993-YRjy)

- [Biologist perspective](https://docs.google.com/document/d/1UHD6IG9Rti2tD77zwHfSyh5XyTbnJ25S/edit)
  - Single-cell lineage capture across genomic modalities with `CellTag-multi` reveals fate-specific gene regulatory changes -> use of **single-cell lineage-tracing (scLT)**. 
  - High-resolution, noninvasive single-cell lineage tracing in mice and humans based on DNA methylation epi-mutations.
  
- Ming Tommy Tang lists in [Single-cell LinkedIn post](https://www.linkedin.com/posts/%F0%9F%8E%AF-ming-tommy-tang-40650014_pylemur-activity-7320442513671008256-qLJC) trendy papers and tools for multi-sample, single-cell RNAseq differential expression analysis.

### Correct for Batch Effects

- [`DESeq2-MultiBatch`](https://www.biorxiv.org/content/10.1101/2025.04.20.649392v1): Batch Correction for Multi-Factorial RNA-seq Experiments, avalaible as open-source [GH repository](https://github.com/julienroyulaval/DESeq2-MultiBatch), from @roy2025. 

### Drug-response

- [`TRADE`: Transcriptome-wide analysis of differential expression in perturbation atlases](https://www.nature.com/articles/s41588-025-02169-3), from @nadig2025ng. The paper notably reports how to model a drug-dose response curve with a **Hill equation**. Avalaible as an open-source GH repository [`TRADEtools`](https://github.com/ajaynadig/TRADEtools). [R `TRADE` Tutorial](https://www.rna-seqblog.com/trade-ranscriptome-wide-analysis-of-differential-expression/)

## Barcode Differential Analysis

### `bartools` and `BARtab`

- Analysis of synthetic cellular barcodes in the genome and transcriptome with `BARtab` and `bartools`. 

### `DEBRA`

- Pros `DEBRA`
  - Better characterisation of the mean-variance deviation -> between `trended` or `shrinkage`, `trended` is favoured.

- Cons `DEBRA`:
  - `DEBRA` does not account for outliers expression, nor zero-inflated counts -> recommendation of `glmQLFit` and `glmQLFTest` for routine GLM-based DE analyses, from [`EdgeR`: Explaning dispersion types to newbies](https://support.bioconductor.org/p/110273/).
  - Complex protocol for discarding lowly differentially expressed barcodes.
  - No available BioConductor/CRAN Repository, while latest [`DEBRA` GitHub update](https://github.com/YevhenAkimov/DEBRA_1.01) dates back more than 4 years.

## Drug clustering and mapping

- Use of graph clustering approaches? Like Louvain? + multiple case studies, how to combine them (2 vials of cell lines)?
- Compare with ATC prediction and clustering: [`PDATC-NCPMKL-updated` GH Repo](https://github.com/Lywhere/PDATC-NCPMKL-updated).

# Appendix {.appendix}

## Repository organisation and Data tracking

- [Project Code Organisation](https://www.linkedin.com/posts/danleedata_become-10-better-at-machine-learning-by-activity-7320797963549564928-Au1_)

- [Data Fingerprinting with `MDA5` and Environment Snapshots: R and Code tracking](https://www.linkedin.com/posts/sebastian-rauschert-836760a0_datascience-reproducibleanalytics-commandline-activity-7312256950962896896-9JkE)

- [`DataLad` OR `DVC` // `Git LFS` or `Alembic`](https://www.linkedin.com/posts/sebastian-rauschert-836760a0_install-datalad-activity-7307188424682127361-dJ_G), (https://www.linkedin.com/posts/sebastian-rauschert-836760a0_dataversioncontrol-dvc-datascience-activity-7314768876876058626-GVNF), (https://www.linkedin.com/posts/sebastian-rauschert-836760a0_reproducibility-databaseversioning-dataengineering-activity-7315522466259390466-zMun)

- [Homogenise `Excel` colnames](https://www.linkedin.com/posts/%F0%9F%8E%AF-ming-tommy-tang-40650014_115-every-wet-lab-uses-spreadsheets-activity-7319725287603130368-9ILC)

## Publish cell lines on Web repositories


- [`cellosaurus` General Comments](https://www.expasy.org/resources/cellosaurus) and [`cellosaurus` Official Website](https://www.cellosaurus.org/), from[@bairoch2018jbt; @robin2020ijc]. Specifically, the `CLASTR` algorithm relies on *short-tandem repeat* patterns to map user-provided cell lines with the ones available in `Cellosaurus` database.
- [`Bgee`](https://www.bgee.org/), from [@bastian2021nar; @bastian2025nar].

<!-- ## Mapping short and full samples IDs {#sec-ID-mapping} -->

<!-- - [`fuzzyjoin`: Join data frames on inexact matching](https://github.com/dgrtwo/fuzzyjoin) uses Levenshtein/cosine/Jaccard distance for mapping closely related IDs (may account for spurious discrepancies) -->

